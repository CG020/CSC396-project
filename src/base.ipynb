{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pieces from HW3 for Reference\n",
    "\n",
    "## This will not run this is just a skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling of the data csvs - we will need to add the csv documents themselves into proper folders, i created a labels.txt for the labels that will be gathered from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train/thefile', header=None)\n",
    "\n",
    "# these are the columsn that are categorized going through the contents of the file\n",
    "# we need to develop the parsing structure\n",
    "\n",
    "train_df.columns = ['class index', 'title', 'description']\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels = open('data/ag_news_csv/labels.txt').read().splitlines()\n",
    "classes = train_df['class index'].map(lambda i: labels[i-1])\n",
    "train_df.insert(1, 'class', classes)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# this is the bar plot that looks at the balance of the categorization\n",
    "# for our data - for us it could be divided by symptom appearrance or someth\n",
    "\n",
    "pd.value_counts(train_df['class']).plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next part of the file is some line parsing to get rid of odd characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.loc[0, 'description'])\n",
    "\n",
    "train_df['text'] = train_df['title'].str.lower() + \" \" + train_df['description'].str.lower()\n",
    "train_df['text'] = train_df['text'].str.replace('\\\\', ' ', regex=False)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['tokens'] = train_df['text'].progress_map(word_tokenize)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary creation - repeated words that must hold significance in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, dev_df = train_test_split(train_df, train_size=0.8)\n",
    "train_df.reset_index(inplace=True)\n",
    "dev_df.reset_index(inplace=True)\n",
    "\n",
    "print(f'train rows: {len(train_df.index):,}')\n",
    "print(f'dev rows: {len(dev_df.index):,}')\n",
    "\n",
    "threshold = 10\n",
    "tokens = train_df['tokens'].explode().value_counts()\n",
    "tokens = tokens[tokens > threshold]\n",
    "id_to_token = ['[UNK]'] + tokens.index.tolist()\n",
    "token_to_id = {w:i for i,w in enumerate(id_to_token)}\n",
    "vocabulary_size = len(id_to_token)\n",
    "print(f'vocabulary size: {vocabulary_size:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_feature_vector(tokens, unk_id=0):\n",
    "    vector = defaultdict(int)\n",
    "    for t in tokens:\n",
    "        i = token_to_id.get(t, unk_id)\n",
    "        vector[i] += 1\n",
    "    return vector\n",
    "\n",
    "train_df['features'] = train_df['tokens'].progress_map(make_feature_vector)\n",
    "dev_df['features'] = dev_df['tokens'].progress_map(make_feature_vector)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting on model creation - comments from assign throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.zeros(vocabulary_size, dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[index])\n",
    "        for k,v in self.x[index].items():\n",
    "            x[k] = v\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "batch_size = 500\n",
    "shuffle = True\n",
    "n_epochs = 5\n",
    "input_dim = vocabulary_size\n",
    "hidden_dim = 50\n",
    "output_dim = len(labels)\n",
    "dropout = 0.3\n",
    "\n",
    "# initialize the model, loss function, optimizer, and data-loader\n",
    "model = Model(input_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay)\n",
    "train_ds = MyDataset(\n",
    "    train_df['features'],\n",
    "    train_df['class index'] - 1)\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle)\n",
    "dev_ds = MyDataset(\n",
    "    dev_df['features'],\n",
    "    dev_df['class index'] - 1)\n",
    "dev_dl = DataLoader(\n",
    "    dev_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle)\n",
    "\n",
    "# lists used to store plotting data\n",
    "train_loss, train_acc = [], []\n",
    "dev_loss, dev_acc = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "for epoch in range(n_epochs):\n",
    "    losses, acc = [], []\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    for X, y_true in tqdm(train_dl, desc=f'epoch {epoch+1} (train)'):\n",
    "        # clear gradients\n",
    "        model.zero_grad()\n",
    "        # send batch to right device\n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        # predict label scores\n",
    "        y_pred = model(X)\n",
    "        # compute loss\n",
    "        loss = loss_func(y_pred, y_true)\n",
    "        # compute accuracy\n",
    "        gold = y_true.detach().cpu().numpy()\n",
    "        pred = np.argmax(y_pred.detach().cpu().numpy(), axis=1)\n",
    "        # accumulate for plotting\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        acc.append(accuracy_score(gold, pred))\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # optimize model parameters\n",
    "        optimizer.step()\n",
    "    # save epoch stats\n",
    "    train_loss.append(np.mean(losses))\n",
    "    train_acc.append(np.mean(acc))\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    # disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        losses, acc = [], []\n",
    "        for X, y_true in tqdm(dev_dl, desc=f'epoch {epoch+1} (dev)'):\n",
    "            # send batch to right device\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            # predict label scores\n",
    "            y_pred = model(X)\n",
    "            # compute loss\n",
    "            loss = loss_func(y_pred, y_true)\n",
    "            # compute accuracy\n",
    "            gold = y_true.cpu().numpy()\n",
    "            pred = np.argmax(y_pred.cpu().numpy(), axis=1)\n",
    "            # accumulate for plotting\n",
    "            losses.append(loss.cpu().item())\n",
    "            acc.append(accuracy_score(gold, pred))\n",
    "        # save epoch stats\n",
    "        dev_loss.append(np.mean(losses))\n",
    "        dev_acc.append(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting for visualization of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(n_epochs) + 1\n",
    "\n",
    "plt.plot(x, train_loss)\n",
    "plt.plot(x, dev_loss)\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(x, train_acc)\n",
    "plt.plot(x, dev_acc)\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# repeat all preprocessing done above, this time on the test set\n",
    "test_df = pd.read_csv('data/ag_news_csv/test.csv', header=None)\n",
    "test_df.columns = ['class index', 'title', 'description']\n",
    "test_df['text'] = test_df['title'].str.lower() + \" \" + test_df['description'].str.lower()\n",
    "test_df['text'] = test_df['text'].str.replace('\\\\', ' ', regex=False)\n",
    "test_df['tokens'] = test_df['text'].progress_map(word_tokenize)\n",
    "test_df['features'] = test_df['tokens'].progress_map(make_feature_vector)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "dataset = MyDataset(test_df['features'], test_df['class index'] - 1)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "y_pred = []\n",
    "\n",
    "# disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for X, _ in tqdm(data_loader):\n",
    "        X = X.to(device)\n",
    "        # predict one class per example\n",
    "        y = torch.argmax(model(X), dim=1)\n",
    "        # convert tensor to numpy array\n",
    "        y_pred.append(y.cpu().numpy())\n",
    "    \n",
    "# print results\n",
    "y_true = dataset.y\n",
    "y_pred = np.concatenate(y_pred)\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "disp.plot(cmap='Blues', values_format='.2f', colorbar=False, ax=ax, xticks_rotation=45)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
